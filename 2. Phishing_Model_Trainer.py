import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score
from sklearn.preprocessing import StandardScaler, LabelEncoder
from xgboost import XGBClassifier
import joblib
import os
from datetime import datetime
import sys
import io
import time
import warnings
from collections import OrderedDict
warnings.filterwarnings('ignore')

class OptimizedPhishingDetector:
    def __init__(self):
        self.models = {}
        self.scaler = StandardScaler()
        self.label_encoder = LabelEncoder()
        self.feature_columns = []
        self.best_model = None
        self.best_model_name = ""
        self.performance_metrics = {}
        
    def load_collected_data(self, data_dir=None):
        """ì²« ë²ˆì§¸ ìŠ¤í¬ë¦½íŠ¸ë¡œ ìˆ˜ì§‘ëœ ë°ì´í„° ë¡œë“œ"""
        if data_dir is None:
            # ê°€ì¥ ìµœê·¼ ìƒì„±ëœ phishing_data í´ë” ì°¾ê¸°
            current_dir = os.getcwd()
            phishing_dirs = [d for d in os.listdir(current_dir) 
                            if d.startswith('phishing_data') and os.path.isdir(d)]
            if not phishing_dirs:
                print("X ìˆ˜ì§‘ëœ ë°ì´í„° í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                print(f"í˜„ì¬ ë””ë ‰í† ë¦¬: {current_dir}")
                print(f"ì‚¬ìš© ê°€ëŠ¥í•œ í´ë”ë“¤: {[d for d in os.listdir(current_dir) if os.path.isdir(d)]}")
                return False
            # ê°€ì¥ ìµœê·¼ í´ë” ì„ íƒ (ë‚ ì§œ ê¸°ì¤€)
            data_dir = max(phishing_dirs)
                
            print(f" ë°ì´í„° ë””ë ‰í† ë¦¬: {data_dir}")
            
            try:
                # URL ë°ì´í„°ì…‹ ë¡œë“œ
                url_file = os.path.join(data_dir, 'url_dataset_with_features.csv')
                if os.path.exists(url_file):
                    self.url_data = pd.read_csv(url_file)
                    print(f" URL ë°ì´í„° ë¡œë“œ: {self.url_data.shape}")
                else:
                    print("X URL í”¼ì²˜ ë°ì´í„°ì…‹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    return False
                
                # ì´ë©”ì¼ ë°ì´í„°ì…‹ ë¡œë“œ (ì„ íƒì‚¬í•­)
                email_file = os.path.join(data_dir, 'email_samples.csv')
                if os.path.exists(email_file):
                    self.email_data = pd.read_csv(email_file)
                    print(f" ì´ë©”ì¼ ë°ì´í„° ë¡œë“œ: {self.email_data.shape}")
                
                return True
                
            except Exception as e:
                print(f"X ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
                return False
    
    def preprocess_data(self):
        """ë°ì´í„° ì „ì²˜ë¦¬ ë° í”¼ì²˜ ì¤€ë¹„"""
        print("\n ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘...")
        
        # ê²°ì¸¡ì¹˜ ë° ì¤‘ë³µ ì œê±°
        initial_size = len(self.url_data)
        self.url_data = self.url_data.dropna()
        self.url_data = self.url_data.drop_duplicates(subset=['url'])
        
        print(f"   ì „ì²˜ë¦¬ í›„: {len(self.url_data)}ê°œ ({initial_size - len(self.url_data)}ê°œ ì œê±°)")
        
        # í”¼ì²˜ ì»¬ëŸ¼ ì„ íƒ
        exclude_columns = {
            'url', 'label', 'source', 'phish_id', 'phish_detail_url', 
            'submission_time', 'verified', 'online', 'collection_time',
            'url_status', 'threat', 'tags'
        }
        
        self.feature_columns = [col for col in self.url_data.columns 
                               if col not in exclude_columns and 
                               self.url_data[col].dtype in ['int64', 'float64', 'bool']]
        
        print(f"   ì„ íƒëœ í”¼ì²˜: {len(self.feature_columns)}ê°œ")
        print(f"   í”¼ì²˜ ëª©ë¡: {self.feature_columns}")
        
        # í”¼ì²˜ì™€ íƒ€ê²Ÿ ë¶„ë¦¬
        X = self.url_data[self.feature_columns].fillna(0)
        y = self.url_data['label']
        
        # ë ˆì´ë¸” ì¸ì½”ë”©
        y_encoded = self.label_encoder.fit_transform(y)
        
        # í´ë˜ìŠ¤ ë¶„í¬ í™•ì¸
        unique_labels, counts = np.unique(y, return_counts=True)
        print(f"   í´ë˜ìŠ¤ ë¶„í¬: {dict(zip(unique_labels, counts))}")
        
        return X, y_encoded
    
    def train_models(self, X, y):
        """ë‹¤ì¤‘ ëª¨ë¸ í•™ìŠµ ë° í‰ê°€"""
        print("\n ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
        
        # ë°ì´í„° ë¶„í• 
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        
        # ë°ì´í„° ìŠ¤ì¼€ì¼ë§
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_test_scaled = self.scaler.transform(X_test)
        
        # ëª¨ë¸ ì •ì˜
        models_config = {
            'RandomForest': {
                'model': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),
                'use_scaling': False
            },
            'GradientBoosting': {
                'model': GradientBoostingClassifier(random_state=42),
                'use_scaling': False
            },
            'XGBoost': {  # ì¶”ê°€
                'model': XGBClassifier(random_state=42, 
                                       # use_label_encoder=False, 
                                       eval_metric='logloss'),
                'use_scaling': False
            },
            'LogisticRegression': {
                'model': LogisticRegression(random_state=42, max_iter=20000),
                'use_scaling': True
            },
            'NeuralNetwork': {
                'model': MLPClassifier(hidden_layer_sizes=(100, 50), random_state=42, max_iter=500),
                'use_scaling': True
            }
        }
        
        best_score = 0
        
        # ëª…ì‹œì  í•™ìŠµ ìˆœì„œ ì§€ì •
        model_order = ['GradientBoosting', 'RandomForest', 'NeuralNetwork', 'LogisticRegression']
        
        for name, config in models_config.items():
            print(f"\n   {name} í•™ìŠµ ì¤‘...")
            
            model = config['model']
            use_scaling = config['use_scaling']
            
            # ëª¨ë¸ í•™ìŠµ
            if use_scaling:
                model.fit(X_train_scaled, y_train)
                y_pred = model.predict(X_test_scaled)
                y_pred_proba = model.predict_proba(X_test_scaled)
                cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='accuracy')
            else:
                model.fit(X_train, y_train)
                y_pred = model.predict(X_test)
                y_pred_proba = model.predict_proba(X_test)
                cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')
            
            # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°
            accuracy = accuracy_score(y_test, y_pred)
            precision = precision_score(y_test, y_pred, average='weighted')
            recall = recall_score(y_test, y_pred, average='weighted')
            f1 = f1_score(y_test, y_pred, average='weighted')
            cv_mean = cv_scores.mean()
            cv_std = cv_scores.std()
            
            # ê²°ê³¼ ì €ì¥
            self.models[name] = {
                'model': model,
                'use_scaling': use_scaling,
                'accuracy': accuracy,
                'precision': precision,
                'recall': recall,
                'f1': f1,
                'cv_mean': cv_mean,
                'cv_std': cv_std,
                'predictions': y_pred,
                'probabilities': y_pred_proba
            }
            
            print(f"      ì •í™•ë„: {accuracy:.4f}")
            print(f"      CV ì ìˆ˜: {cv_mean:.4f} (Â±{cv_std:.4f})")
            print(f"      F1 ì ìˆ˜: {f1:.4f}")
            
            # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì—…ë°ì´íŠ¸
            if cv_mean > best_score:
                best_score = cv_mean
                self.best_model_name = name
                self.best_model = model
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì €ì¥ (ì˜ˆì¸¡ìš©)
        self.X_test = X_test
        self.X_test_scaled = X_test_scaled
        self.y_test = y_test
        
        print(f"\n ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {self.best_model_name}")
        print(f"   CV ì ìˆ˜: {self.models[self.best_model_name]['cv_mean']:.4f}")
        
        return self.models
    
    def optimize_best_model(self):
        """ìµœê³  ì„±ëŠ¥ ëª¨ë¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”"""
        print(f"\n {self.best_model_name} í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”...")
        
        X, y = self.preprocess_data()
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        
        # ëª¨ë¸ë³„ í•˜ì´í¼íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ
        param_grids = {
            'RandomForest': {
                'n_estimators': [50, 100, 200],
                'max_depth': [10, 20, None],
                'min_samples_split': [2, 5],
                'min_samples_leaf': [1, 2]
            },
            'GradientBoosting': {
                'n_estimators': [100, 200, 300],
                'learning_rate': [0.01, 0.05, 0.1],
                'max_depth': [3, 5, 7],
                'min_samples_split': [2, 5],
                'min_samples_leaf': [1, 2],
                'subsample': [0.8, 1.0]
            },
            'XGBoost': {
                'n_estimators': [100, 300, 500,700],
                'learning_rate': [0.01, 0.05, 0.1, 0.2],
                'max_depth': [3, 5, 7, 9],
                'subsample': [0.7, 0.8, 1.0],
                'colsample_bytree': [0.7, 0.8, 1.0],
                'gamma': [0, 1],
                'reg_lambda': [1, 5],  # L2 ì •ê·œí™”
                'reg_alpha': [0, 1]    # L1 ì •ê·œí™”
            },
            'LogisticRegression': {
                'C': [0.1, 1, 10, 100],
                'penalty': ['l1', 'l2'],
                'solver': ['liblinear', 'saga']
            },
            'NeuralNetwork': {
                'hidden_layer_sizes': [(50,), (100,), (100, 50), (200, 100)],
                'alpha': [0.001, 0.01, 0.1],
                'learning_rate': ['constant', 'adaptive']
            }
        }
        
        if self.best_model_name in param_grids:
            param_grid = param_grids[self.best_model_name]
            base_model = self.models[self.best_model_name]['model']
            use_scaling = self.models[self.best_model_name]['use_scaling']
            
            # GridSearchCV ì‹¤í–‰
            grid_search = GridSearchCV(
                base_model.__class__(**base_model.get_params()),
                param_grid,
                cv=3,
                scoring='accuracy',
                n_jobs=-1,
                verbose=2
            )
            
            if use_scaling:
                X_train_scaled = self.scaler.fit_transform(X_train)
                grid_search.fit(X_train_scaled, y_train)
            else:
                grid_search.fit(X_train, y_train)
            
            # ìµœì í™”ëœ ëª¨ë¸ë¡œ ì—…ë°ì´íŠ¸
            self.best_model = grid_search.best_estimator_
            self.models[self.best_model_name]['model'] = self.best_model
            
            print(f"   ìµœì  íŒŒë¼ë¯¸í„°: {grid_search.best_params_}")
            print(f"   ìµœì  ì ìˆ˜: {grid_search.best_score_:.4f}")
            
            return grid_search.best_estimator_
        
        return self.best_model
    
    def evaluate_final_model(self):
        """ìµœì¢… ëª¨ë¸ ì„±ëŠ¥ í‰ê°€"""
        print(f"\n {self.best_model_name} ìµœì¢… ì„±ëŠ¥ í‰ê°€...")
        
        use_scaling = self.models[self.best_model_name]['use_scaling']
        
        if use_scaling:
            y_pred = self.best_model.predict(self.X_test_scaled)
        else:
            y_pred = self.best_model.predict(self.X_test)
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°
        accuracy = accuracy_score(self.y_test, y_pred)
        precision = precision_score(self.y_test, y_pred, average='weighted')
        recall = recall_score(self.y_test, y_pred, average='weighted')
        f1 = f1_score(self.y_test, y_pred, average='weighted')
        
        self.performance_metrics = {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1_score': f1
        }
        
        print(f"   ì •í™•ë„: {accuracy:.4f}")
        print(f"   ì •ë°€ë„: {precision:.4f}")
        print(f"   ì¬í˜„ìœ¨: {recall:.4f}")
        print(f"   F1 ì ìˆ˜: {f1:.4f}")
        
        # í´ë˜ìŠ¤ë³„ ì„±ëŠ¥
        print("\n   í´ë˜ìŠ¤ë³„ ì„±ëŠ¥:")
        report = classification_report(self.y_test, y_pred, 
                                     target_names=self.label_encoder.classes_,
                                     output_dict=True)
        
        for class_name in self.label_encoder.classes_:
            if class_name in report:
                metrics = report[class_name]
                print(f"   {class_name}:")
                print(f"     ì •ë°€ë„: {metrics['precision']:.4f}")
                print(f"     ì¬í˜„ìœ¨: {metrics['recall']:.4f}")
                print(f"     F1 ì ìˆ˜: {metrics['f1-score']:.4f}")
        
        return self.performance_metrics
    
    def save_model(self, filename=None):
        """ëª¨ë¸ ë° ê´€ë ¨ ë°ì´í„° ì €ì¥"""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"phishing_detector.pkl"
        
        model_data = {
            'model': self.best_model,
            'model_name': self.best_model_name,
            'scaler': self.scaler,
            'label_encoder': self.label_encoder,
            'feature_columns': self.feature_columns,
            'performance_metrics': self.performance_metrics,
            'use_scaling': self.models[self.best_model_name]['use_scaling'],
            'training_timestamp': datetime.now().isoformat()
        }
        
        joblib.dump(model_data, filename)
        print(f"\nğŸ’¾ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {filename}")
        print(f"   ëª¨ë¸: {self.best_model_name}")
        print(f"   ì„±ëŠ¥: {self.performance_metrics.get('accuracy', 0):.4f}")
        
        return filename
    
    def load_model(self, filename):
        """ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ"""
        try:
            model_data = joblib.load(filename)
            
            self.best_model = model_data['model']
            self.best_model_name = model_data['model_name']
            self.scaler = model_data['scaler']
            self.label_encoder = model_data['label_encoder']
            self.feature_columns = model_data['feature_columns']
            self.performance_metrics = model_data.get('performance_metrics', {})
            
            print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {filename}")
            print(f"   ëª¨ë¸: {self.best_model_name}")
            
            return True
            
        except Exception as e:
            print(f" ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    
    def predict_single_url(self, url_features):
        """ë‹¨ì¼ URL ì˜ˆì¸¡"""
        if self.best_model is None:
            print(" í•™ìŠµëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        try:
            # í”¼ì²˜ DataFrame ìƒì„±
            features_df = pd.DataFrame([url_features])
            features_df = features_df.reindex(columns=self.feature_columns, fill_value=0)
            
            # ì˜ˆì¸¡ ì‹¤í–‰
            use_scaling = self.models.get(self.best_model_name, {}).get('use_scaling', False)
            
            if use_scaling:
                features_scaled = self.scaler.transform(features_df)
                prediction = self.best_model.predict(features_scaled)[0]
                probability = self.best_model.predict_proba(features_scaled)[0]
            else:
                prediction = self.best_model.predict(features_df)[0]
                probability = self.best_model.predict_proba(features_df)[0]
            
            # ê²°ê³¼ ë³€í™˜
            predicted_label = self.label_encoder.inverse_transform([prediction])[0]
            confidence = max(probability)
            
            # í”¼ì‹± í™•ë¥  ê³„ì‚° (í´ë˜ìŠ¤ ìˆœì„œì— ë”°ë¼)
            phishing_prob = probability[1] if len(probability) > 1 else (
                probability[0] if self.label_encoder.classes_[0] == 'phishing' else 1 - probability[0]
            )
            
            result = {
                'prediction': predicted_label,
                'confidence': confidence,
                'phishing_probability': phishing_prob,
                'legitimate_probability': 1 - phishing_prob
            }
            
            return result
            
        except Exception as e:
            print(f" ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
            return None
    
    def batch_predict(self, urls_features):
        """ë°°ì¹˜ URL ì˜ˆì¸¡"""
        if self.best_model is None:
            print(" í•™ìŠµëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        try:
            # í”¼ì²˜ DataFrame ìƒì„±
            features_df = pd.DataFrame(urls_features)
            features_df = features_df.reindex(columns=self.feature_columns, fill_value=0)
            
            # ì˜ˆì¸¡ ì‹¤í–‰
            use_scaling = self.models.get(self.best_model_name, {}).get('use_scaling', False)
            
            if use_scaling:
                features_scaled = self.scaler.transform(features_df)
                predictions = self.best_model.predict(features_scaled)
                probabilities = self.best_model.predict_proba(features_scaled)
            else:
                predictions = self.best_model.predict(features_df)
                probabilities = self.best_model.predict_proba(features_df)
            
            # ê²°ê³¼ ë³€í™˜
            predicted_labels = self.label_encoder.inverse_transform(predictions)
            
            results = []
            for i, (pred, prob) in enumerate(zip(predicted_labels, probabilities)):
                confidence = max(prob)
                phishing_prob = prob[1] if len(prob) > 1 else (
                    prob[0] if self.label_encoder.classes_[0] == 'phishing' else 1 - prob[0]
                )
                
                results.append({
                    'prediction': pred,
                    'confidence': confidence,
                    'phishing_probability': phishing_prob,
                    'legitimate_probability': 1 - phishing_prob
                })
            
            return results
            
        except Exception as e:
            print(f" ë°°ì¹˜ ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
            return None
    
    def run_full_training(self, data_dir=None, optimize=True):

        start_time = time.time()  # ì´ ì¤„ì´ ì—†ìœ¼ë©´ NameError ë°œìƒ

        # ì „ì²´ í•™ìŠµ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        print(" AI í”¼ì‹± íƒì§€ ëª¨ë¸ í•™ìŠµ ì‹œì‘")
        print("=" * 50)
           
        # 1. ë°ì´í„° ë¡œë“œ
        if not self.load_collected_data(data_dir):
            return False
            
        # 2. ë°ì´í„° ì „ì²˜ë¦¬
        X, y = self.preprocess_data()
        
        # 3. ëª¨ë¸ í•™ìŠµ
        self.train_models(X, y)

        # ìê¾¸ë§Œ ì„ í˜•íšŒê·€ë¥¼ ì„ íƒí•˜ê¸¸ë˜ XGBoostë¥¼ ê°•ì œë¡œ ì„ íƒ
        self.best_model_name = 'XGBoost'
        self.best_model = self.models['XGBoost']['model']

        # 4. í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” (ì„ íƒì‚¬í•­)
        if optimize:
            self.optimize_best_model()
            
        # 5. ìµœì¢… í‰ê°€
        self.evaluate_final_model()
            
        # 6. ëª¨ë¸ ì €ì¥
        model_file = self.save_model()

        end_time = time.time()
        duration = end_time - start_time
            
        print("\n ğŸ‰ í•™ìŠµ ì™„ë£Œ!")
        print(f" ğŸ“ ëª¨ë¸ íŒŒì¼: {model_file}")
        print(f" â° ì´ í•™ìŠµ ì‹œê°„: {duration:.2f}ì´ˆ")
        print(f" ğŸ•“ ì™„ë£Œ ì‹œê°: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(end_time))}")

            
        return model_file


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    detector = OptimizedPhishingDetector()
    
    # ì „ì²´ í•™ìŠµ ì‹¤í–‰
    model_file = detector.run_full_training(optimize=True)
    
    if model_file:
        print(f"\nâœ¨ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {model_file}")
        
        # ì˜ˆì¸¡ ì˜ˆì‹œ
        print("\n ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸:")
        sample_features = {
            'url_length': 85,
            'domain_length': 15,
            'num_dots': 3,
            'num_hyphens': 1,
            'num_underscores': 0,
            'num_percent': 0,
            'num_query_params': 2,
            'has_ip': False,
            'has_https': True,
            'subdomain_count': 2
        }
        
        result = detector.predict_single_url(sample_features)
        if result:
            print(f"   ì˜ˆì¸¡: {result['prediction']}")
            print(f"   ì‹ ë¢°ë„: {result['confidence']:.3f}")
            print(f"   í”¼ì‹± í™•ë¥ : {result['phishing_probability']:.3f}")

"""
import joblib
model_data = joblib.load("phishing_detector.pkl")
print("ìµœì¢… ëª¨ë¸:", model_data['model_name'])
"""

if __name__ == "__main__":

    # ì¶œë ¥ ìº¡ì²˜ë¥¼ ìœ„í•œ ìŠ¤íŠ¸ë¦¼ ìƒì„±
    original_stdout = sys.stdout
    captured_output = io.StringIO()
    sys.stdout = captured_output

    # main ì‹¤í–‰
    main()

    # ìº¡ì²˜ëœ ì¶œë ¥ ë‚´ìš©ì„ HTMLë¡œ ì €ì¥
    sys.stdout = original_stdout
    html_content = captured_output.getvalue().replace('\n', '<br>\n')

    with open("training_output_log.html", "w", encoding="utf-8") as html_file:
        html_file.write("<html><body style='font-family:monospace;'>\n")
        html_file.write(html_content)
        html_file.write("\n</body></html>")

    print("âœ… í„°ë¯¸ë„ ì¶œë ¥ì´ 'training_output_log.html' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")


#if __name__ == "__main__":
#    main()