"""
AI í”¼ì‹± íƒì§€ ì‹œìŠ¤í…œìš© ë°ì´í„°ì…‹ ìˆ˜ì§‘ê¸°
PyCharmì—ì„œ ì‹¤í–‰ ê°€ëŠ¥í•œ ì›¹ ìŠ¤í¬ë˜í•‘ ì½”ë“œ

í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜:
pip install requests beautifulsoup4 pandas lxml kaggle openpyxl tqdm
"""

import requests 
import pandas as pd
import json
import time
import os
from datetime import datetime
from urllib.parse import urlparse, urljoin
from bs4 import BeautifulSoup
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

class PhishingDataCollector:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive'
        })
        
    def create_output_directory(self):
        """ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_dir = f"phishing_data"
        os.makedirs(output_dir, exist_ok=True)
        return output_dir
    
    def collect_phishtank_data(self):
        """PhishTank ê³µê°œ ë°ì´í„° ìˆ˜ì§‘"""
        print("ğŸ“Š PhishTank ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
        try:
            # PhishTank ê³µê°œ ë°ì´í„°ë² ì´ìŠ¤ URL
            url = "http://data.phishtank.com/data/online-valid.json"
            
            print("   ë‹¤ìš´ë¡œë“œ ì‹œì‘...")
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            
            data = response.json()
            print(f"   âœ… {len(data)}ê°œì˜ í”¼ì‹± URL ìˆ˜ì§‘ ì™„ë£Œ")
            
            # DataFrameìœ¼ë¡œ ë³€í™˜
            df = pd.DataFrame(data)
            
            # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ
            if not df.empty:
                df = df[['phish_id', 'url', 'phish_detail_url', 'submission_time', 'verified', 'online']]
                df['label'] = 'phishing'
                df['source'] = 'phishtank'
                
            return df
            
        except Exception as e:
            print(f"   âŒ PhishTank ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return pd.DataFrame()
    
    def collect_openphish_data(self):
        """OpenPhish ë°ì´í„° ìˆ˜ì§‘"""
        print("ğŸ“Š OpenPhish ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
        try:
            url = "https://openphish.com/feed.txt"
            
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            
            urls = response.text.strip().split('\n')
            urls = [url.strip() for url in urls if url.strip()]
            
            print(f"   âœ… {len(urls)}ê°œì˜ í”¼ì‹± URL ìˆ˜ì§‘ ì™„ë£Œ")
            
            # DataFrameìœ¼ë¡œ ë³€í™˜
            df = pd.DataFrame({
                'url': urls,
                'label': 'phishing',
                'source': 'openphish',
                'collection_time': datetime.now().isoformat()
            })
            
            return df
            
        except Exception as e:
            print(f"   âŒ OpenPhish ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return pd.DataFrame()
    
    def collect_urlhaus_data(self):
        """URLhaus ë©€ì›¨ì–´ URL ë°ì´í„° ìˆ˜ì§‘"""
        print("ğŸ“Š URLhaus ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
        try:
            url = "https://urlhaus.abuse.ch/downloads/csv_recent/"
            
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            
            # CSV ë°ì´í„° íŒŒì‹±
            from io import StringIO
            csv_data = StringIO(response.text)
            
            # ì£¼ì„ ë¼ì¸ ì œê±°
            lines = [line for line in csv_data.getvalue().split('\n') if not line.startswith('#')]
            clean_csv = '\n'.join(lines)
            
            df = pd.read_csv(StringIO(clean_csv))
            
            if not df.empty:
                # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ
                df = df[['url', 'url_status', 'threat', 'tags']]
                df['label'] = 'malicious'
                df['source'] = 'urlhaus'
                
            print(f"   âœ… {len(df)}ê°œì˜ ì•…ì„± URL ìˆ˜ì§‘ ì™„ë£Œ")
            return df
            
        except Exception as e:
            print(f"   âŒ URLhaus ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return pd.DataFrame()
    
    def collect_alexa_top_sites(self, count=1000):
        """ì •ìƒ ì›¹ì‚¬ì´íŠ¸ URL ìˆ˜ì§‘ (ëŒ€ì²´ ë°©ë²•)"""
        print("ğŸ“Š ì •ìƒ ì›¹ì‚¬ì´íŠ¸ URL ìˆ˜ì§‘ ì¤‘...")
        try:
            # ê³µê°œëœ ì¸ê¸° ì›¹ì‚¬ì´íŠ¸ ë¦¬ìŠ¤íŠ¸ í™œìš©
            normal_sites = [
                'google.com', 'youtube.com', 'facebook.com', 'twitter.com', 'instagram.com',
                'linkedin.com', 'reddit.com', 'wikipedia.org', 'amazon.com', 'netflix.com',
                'microsoft.com', 'apple.com', 'github.com', 'stackoverflow.com', 'medium.com',
                'naver.com', 'daum.net', 'kakao.com', 'coupang.com', '11st.co.kr',
                'gmarket.co.kr', 'interpark.com', 'auction.co.kr', 'yes24.com', 'aladin.co.kr',
                'bbc.com', 'cnn.com', 'nytimes.com', 'guardian.com', 'reuters.com'
            ]
            
            # HTTP/HTTPS ë²„ì „ ìƒì„±
            urls = []
            for site in normal_sites:
                urls.append(f'https://{site}')
                urls.append(f'https://www.{site}')
            
            df = pd.DataFrame({
                'url': urls,
                'label': 'legitimate',
                'source': 'known_good',
                'collection_time': datetime.now().isoformat()
            })
            
            print(f"   âœ… {len(df)}ê°œì˜ ì •ìƒ URL ìƒì„± ì™„ë£Œ")
            return df
            
        except Exception as e:
            print(f"   âŒ ì •ìƒ URL ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return pd.DataFrame()
    
    def extract_url_features(self, df):
        """URLì—ì„œ í”¼ì²˜ ì¶”ì¶œ"""
        print("ğŸ” URL í”¼ì²˜ ì¶”ì¶œ ì¤‘...")
        
        def get_url_features(url):
            try:
                parsed = urlparse(url)
                return {
                    'url_length': len(url),
                    'domain_length': len(parsed.netloc), 
                    'path_length': len(parsed.path), 
                    'num_dots': url.count('.'),
                    'num_hyphens': url.count('-'),
                    'num_underscores': url.count('_'),
                    'num_percent': url.count('%'), 
                    'num_query_params': len(parsed.query.split('&')) if parsed.query else 0, 
                    'has_ip': any(c.isdigit() for c in parsed.netloc.split('.')) and len(parsed.netloc.split('.')) == 4,
                    'has_https': parsed.scheme == 'https',
                    'subdomain_count': len(parsed.netloc.split('.')) - 2 if len(parsed.netloc.split('.')) > 2 else 0 
                }
            except:
                return {}
        
        # URL í”¼ì²˜ ì¶”ì¶œ
        features_list = [] 
        for url in tqdm(df['url'], desc="URL ë¶„ì„"):
            features = get_url_features(url)
            features_list.append(features)
        
        # í”¼ì²˜ë¥¼ DataFrameì— ì¶”ê°€
        features_df = pd.DataFrame(features_list)
        result_df = pd.concat([df, features_df], axis=1)
        
        print(f"   âœ… {len(features_df.columns)}ê°œì˜ URL í”¼ì²˜ ì¶”ì¶œ ì™„ë£Œ")
        return result_df
    
    def collect_spam_email_samples(self):
        """ìŠ¤íŒ¸ ì´ë©”ì¼ ìƒ˜í”Œ ë°ì´í„° ìƒì„±"""
        print("ğŸ“§ ìŠ¤íŒ¸ ì´ë©”ì¼ ìƒ˜í”Œ ë°ì´í„° ìƒì„± ì¤‘...")
        
        # ì‹¤ì œ ìŠ¤íŒ¸ ì´ë©”ì¼ íŒ¨í„´ ê¸°ë°˜ ìƒ˜í”Œ ìƒì„±
        spam_samples = [
            {
                'subject': 'URGENT: Verify Your Account Now!',
                'body': 'Dear Customer, Your account will be suspended unless you verify immediately. Click here: http://suspicious-bank-verify.com',
                'sender': 'security@fake-bank.com',
                'label': 'spam'
            },
            {
                'subject': 'You Won $1,000,000!',
                'body': 'Congratulations! You have won our lottery. Send your personal details to claim your prize.',
                'sender': 'winner@lottery-scam.net',
                'label': 'spam'
            },
            {
                'subject': 'Free iPhone - Limited Time!',
                'body': 'Get your free iPhone now! Just click this link and enter your credit card details.',
                'sender': 'offers@freestuff-scam.com',
                'label': 'spam'
            }
        ]
        
        # ì •ìƒ ì´ë©”ì¼ ìƒ˜í”Œ
        legitimate_samples = [
            {
                'subject': 'Meeting Reminder - Tomorrow 2PM',
                'body': 'Hi team, Just a reminder about our meeting tomorrow at 2PM in the conference room.',
                'sender': 'manager@company.com',
                'label': 'legitimate'
            },
            {
                'subject': 'Your Order Confirmation #12345',
                'body': 'Thank you for your order. Your items will be shipped within 2 business days.',
                'sender': 'orders@legitimate-store.com',
                'label': 'legitimate'
            }
        ]
        
        all_samples = spam_samples + legitimate_samples
        df = pd.DataFrame(all_samples)
        df['source'] = 'generated_samples'
        
        print(f"   âœ… {len(df)}ê°œì˜ ì´ë©”ì¼ ìƒ˜í”Œ ìƒì„± ì™„ë£Œ")
        return df
    
    def save_datasets(self, datasets, output_dir):
        """ë°ì´í„°ì…‹ ì €ì¥"""
        print(f"ğŸ’¾ ë°ì´í„°ì…‹ ì €ì¥ ì¤‘... (ê²½ë¡œ: {output_dir})")
        
        for name, df in datasets.items():
            if not df.empty:
                # CSV íŒŒì¼ë¡œ ì €ì¥
                csv_path = os.path.join(output_dir, f"{name}.csv")
                df.to_csv(csv_path, index=False, encoding='utf-8')
                
                # Excel íŒŒì¼ë¡œ ì €ì¥
                excel_path = os.path.join(output_dir, f"{name}.xlsx")
                df.to_excel(excel_path, index=False)
                
                print(f"   âœ… {name}: {len(df)}í–‰ ì €ì¥ ì™„ë£Œ")
            else:
                print(f"   âš ï¸ {name}: ë°ì´í„° ì—†ìŒ")
    
    def run_collection(self):
        """ì „ì²´ ë°ì´í„° ìˆ˜ì§‘ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        print("ğŸš€ í”¼ì‹± íƒì§€ ë°ì´í„°ì…‹ ìˆ˜ì§‘ ì‹œì‘")
        print("=" * 50)
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        output_dir = self.create_output_directory()
        
        datasets = {}
        
        # 1. PhishTank ë°ì´í„° ìˆ˜ì§‘
        phishtank_df = self.collect_phishtank_data()
        if not phishtank_df.empty:
            datasets['phishtank_urls'] = phishtank_df
        
        # 2. OpenPhish ë°ì´í„° ìˆ˜ì§‘
        openphish_df = self.collect_openphish_data()
        if not openphish_df.empty:
            datasets['openphish_urls'] = openphish_df
        
        # 3. URLhaus ë°ì´í„° ìˆ˜ì§‘
        urlhaus_df = self.collect_urlhaus_data()
        if not urlhaus_df.empty:
            datasets['urlhaus_malicious'] = urlhaus_df
        
        # 4. ì •ìƒ ì›¹ì‚¬ì´íŠ¸ URL ìˆ˜ì§‘
        legitimate_df = self.collect_alexa_top_sites()
        if not legitimate_df.empty:
            datasets['legitimate_urls'] = legitimate_df
        
        # 5. ìŠ¤íŒ¸ ì´ë©”ì¼ ìƒ˜í”Œ ìƒì„±
        email_df = self.collect_spam_email_samples()
        if not email_df.empty:
            datasets['email_samples'] = email_df
        
        # 6. URL ë°ì´í„° í†µí•© ë° í”¼ì²˜ ì¶”ì¶œ
        if 'phishtank_urls' in datasets or 'openphish_urls' in datasets:
            print("\nğŸ”„ URL ë°ì´í„°ì…‹ í†µí•© ì¤‘...")
            combined_urls = []
            
            if 'phishtank_urls' in datasets:
                combined_urls.append(datasets['phishtank_urls'][['url', 'label', 'source']])
            if 'openphish_urls' in datasets:
                combined_urls.append(datasets['openphish_urls'][['url', 'label', 'source']])
            if 'legitimate_urls' in datasets:
                combined_urls.append(datasets['legitimate_urls'][['url', 'label', 'source']])
            
            if combined_urls:
                combined_df = pd.concat(combined_urls, ignore_index=True)
                combined_df = combined_df.drop_duplicates(subset=['url'])
                
                # URL í”¼ì²˜ ì¶”ì¶œ
                featured_df = self.extract_url_features(combined_df)
                datasets['url_dataset_with_features'] = featured_df
        
        # 7. ë°ì´í„°ì…‹ ì €ì¥
        if datasets:
            self.save_datasets(datasets, output_dir)
            
            # ìš”ì•½ ì •ë³´ ì¶œë ¥
            print("\nğŸ“Š ìˆ˜ì§‘ ì™„ë£Œ ìš”ì•½:")
            print("-" * 30)
            total_records = 0
            for name, df in datasets.items():
                count = len(df)
                total_records += count
                print(f"  {name}: {count:,}ê°œ")
            
            print(f"\nì´ {total_records:,}ê°œì˜ ë ˆì½”ë“œ ìˆ˜ì§‘ ì™„ë£Œ!")
            print(f"ì €ì¥ ìœ„ì¹˜: {os.path.abspath(output_dir)}")
            
            # ë°ì´í„°ì…‹ ì‚¬ìš© ì˜ˆì‹œ ì½”ë“œ ìƒì„±
            self.generate_usage_example(output_dir)
            
        else:
            print("âŒ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        return output_dir
    
    def generate_usage_example(self, output_dir):
        """ìˆ˜ì§‘ëœ ë°ì´í„° ì‚¬ìš© ì˜ˆì‹œ ì½”ë“œ ìƒì„±"""
        example_code = '''
# ìˆ˜ì§‘ëœ ë°ì´í„°ì…‹ ì‚¬ìš© ì˜ˆì‹œ
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# ë°ì´í„° ë¡œë“œ
url_data = pd.read_csv('url_dataset_with_features.csv')
email_data = pd.read_csv('email_samples.csv')

# ê¸°ë³¸ í†µê³„ í™•ì¸
print("URL ë°ì´í„°ì…‹ ì •ë³´:")
print(url_data.info())
print("\\në ˆì´ë¸” ë¶„í¬:")
print(url_data['label'].value_counts())

# í”¼ì²˜ ë¶„ì„
numerical_features = ['url_length', 'domain_length', 'num_dots', 'num_hyphens']
for feature in numerical_features:
    if feature in url_data.columns:
        print(f"\\n{feature} - í”¼ì‹± vs ì •ìƒ:")
        print(url_data.groupby('label')[feature].describe())

# ì‹œê°í™”
plt.figure(figsize=(12, 8))
for i, feature in enumerate(numerical_features[:4], 1):
    if feature in url_data.columns:
        plt.subplot(2, 2, i)
        sns.boxplot(data=url_data, x='label', y=feature)
        plt.title(f'{feature} Distribution')
plt.tight_layout()
plt.show()

# ë¨¸ì‹ ëŸ¬ë‹ ì¤€ë¹„
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report

# í”¼ì²˜ì™€ íƒ€ê²Ÿ ë¶„ë¦¬
feature_columns = [col for col in url_data.columns 
                  if col not in ['url', 'label', 'source', 'phish_id', 
                               'phish_detail_url', 'submission_time', 
                               'verified', 'online', 'collection_time']]

X = url_data[feature_columns].fillna(0)
y = url_data['label']

# í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë¶„í• 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# ëª¨ë¸ í•™ìŠµ
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# ì˜ˆì¸¡ ë° í‰ê°€
y_pred = rf_model.predict(X_test)
print("\\në¶„ë¥˜ ì„±ëŠ¥:")
print(classification_report(y_test, y_pred))

# ì¤‘ìš” í”¼ì²˜ í™•ì¸
feature_importance = pd.DataFrame({
    'feature': feature_columns,
    'importance': rf_model.feature_importances_
}).sort_values('importance', ascending=False)

print("\\nì¤‘ìš” í”¼ì²˜ Top 10:")
print(feature_importance.head(10))
'''
        
        example_path = os.path.join(output_dir, "usage_example.py")
        with open(example_path, 'w', encoding='utf-8') as f:
            f.write(example_code)
        
        print(f"   ğŸ“ ì‚¬ìš© ì˜ˆì‹œ ì½”ë“œ ìƒì„±: {example_path}")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    collector = PhishingDataCollector()
    output_dir = collector.run_collection()
 
    print(f"\nğŸ‰ ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
    print(f"ğŸ“ ê²°ê³¼ í™•ì¸: {os.path.abspath(output_dir)}")
    print("\në‹¤ìŒ ë‹¨ê³„:")
    print("1. ìƒì„±ëœ CSV/Excel íŒŒì¼ë“¤ì„ í™•ì¸í•˜ì„¸ìš”")
    print("2. usage_example.pyë¥¼ ì‹¤í–‰í•˜ì—¬ ë°ì´í„° ë¶„ì„ì„ ì‹œì‘í•˜ì„¸ìš”")
    print("3. í•„ìš”ì— ë”°ë¼ ì¶”ê°€ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ì„¸ìš”")

if __name__ == "__main__":
    main()